{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import run_binary_classifier\n",
    "import run_multilabel_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_binary = os.path.join('../../../', 'data/train_binary.csv')\n",
    "test_binary = os.path.join('../../../', 'data/test_clean_binary.csv')\n",
    "\n",
    "train_multilabel = os.path.join('../../../', 'data/train.csv')\n",
    "test_multilabel = os.path.join('../../../', 'data/test_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trb = pd.read_csv(train_binary)\n",
    "tstb = pd.read_csv(test_binary)\n",
    "trm = pd.read_csv(train_multilabel)\n",
    "tstm = pd.read_csv(test_multilabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "trb = trb.values[:, 2:]\n",
    "tstb = tstb.values[:, 2:]\n",
    "trm = trm.values[:, 2:]\n",
    "tstm = tstm.values[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n",
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " ...\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " ...\n",
      " [0 0 0 0 0 0]\n",
      " [1 0 1 0 1 0]\n",
      " [0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(trb.view())\n",
    "print(tstb.view())\n",
    "print(trm.view())\n",
    "print(tstm.view())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0,\n",
       "        'Thank you for understanding. I think very highly of you and would not revert without discussion.',\n",
       "        0],\n",
       "       [1, ':Dear god this site is horrible.', 0],\n",
       "       [2,\n",
       "        '\"::: Somebody will invariably try to add Religion?  Really??  You mean, the way people have invariably kept adding \"\"Religion\"\" to the Samuel Beckett infobox?  And why do you bother bringing up the long-dead completely non-existent \"\"Influences\"\" issue?  You\\'re just flailing, making up crap on the fly. \\n ::: For comparison, the only explicit acknowledgement in the entire Amos Oz article that he is personally Jewish is in the categories!    \\n\\n \"',\n",
       "        0],\n",
       "       ...,\n",
       "       [63975,\n",
       "        '==shame on you all!!!== \\n\\n You want to speak about gays and not about romanians...',\n",
       "        0],\n",
       "       [63976,\n",
       "        'MEL GIBSON IS A NAZI BITCH WHO MAKES SHITTY MOVIES. HE HAS SO MUCH BUTTSEX THAT HIS ASSHOLE IS NOW BIG ENOUGH TO BE CONSIDERED A COUNTRY.',\n",
       "        1],\n",
       "       [63977,\n",
       "        '\" \\n\\n == Unicorn lair discovery == \\n\\n Supposedly, a \\'unicorn lair\\' has been discovered in Pyongyang, North Korea. The lair is supposedly associated with King Dongmyeong of Goguryeo, who supposedly rode a unicorn.  It should be added, but i can\\'t quite find where to insert it. \"',\n",
       "        0]], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tstb.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_param_grid = {\n",
    "        'bag_of_words__stop_words': ['english'],\n",
    "        'bag_of_words__ngram_range': [(1, 2)],\n",
    "        'bag_of_words__max_features': [500],\n",
    "        'dim_reduct__n_components': [300],\n",
    "        'normalizer__norm': ['l2'],\n",
    "        'classifier__C': [5., 10.]\n",
    "}\n",
    "\n",
    "\n",
    "multilabel_param_grid  = [{\n",
    "        'estimator__bag_of_words__stop_words': ['english'],\n",
    "        'estimator__bag_of_words__ngram_range': [(1, 2)],\n",
    "        'estimator__bag_of_words__max_features': [500],\n",
    "        'estimator__dim_reduct__n_components': [300],\n",
    "        'estimator__normalizer__norm': ['l2'],\n",
    "        'estimator__classifier__C': [5., 10.]\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zvonimir/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================  Classification report  =================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.92      0.86      4883\n",
      "          1       0.90      0.78      0.84      4852\n",
      "\n",
      "avg / total       0.86      0.85      0.85      9735\n",
      "\n",
      "'=================     Best parameters     ================='\n",
      "{'bag_of_words__max_features': 500, 'bag_of_words__ngram_range': (1, 2), 'bag_of_words__stop_words': 'english', 'classifier__C': 10.0, 'dim_reduct__n_components': 300, 'normalizer__norm': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "binary_clf = run_binary_classifier.run(binary_param_grid, LogisticRegression(), comments_file=train_binary)\n",
    "\n",
    "with open('./saved_models/log_reg_joint_binary.pkl', 'wb') as saved_model:\n",
    "\tpickle.dump(binary_clf, file=saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 15294\n",
      "1 1595\n",
      "2 8449\n",
      "3 478\n",
      "4 7877\n",
      "5 1405\n",
      "0 5849\n",
      "[15294  1595  8449   478  7877  1405]\n",
      "===================================\n",
      "0 9445\n",
      "1 4254\n",
      "2 8449\n",
      "3 5371\n",
      "4 7877\n",
      "5 4444\n",
      "===================================\n",
      "143346\n",
      "39840\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zvonimir/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 68.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================  Classification report  =================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99     73851\n",
      "          1       0.85      0.76      0.80     25075\n",
      "          2       0.95      0.98      0.97     55793\n",
      "          3       0.84      0.68      0.75      7915\n",
      "          4       0.94      0.99      0.96     69097\n",
      "          5       0.93      0.79      0.85     22484\n",
      "\n",
      "avg / total       0.94      0.94      0.94    254215\n",
      "\n",
      "=================     Best parameters     =================\n",
      "{'estimator__bag_of_words__max_features': 500,\n",
      " 'estimator__bag_of_words__ngram_range': (1, 2),\n",
      " 'estimator__bag_of_words__stop_words': 'english',\n",
      " 'estimator__classifier__C': 10.0,\n",
      " 'estimator__dim_reduct__n_components': 300,\n",
      " 'estimator__normalizer__norm': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "multilabel_clf = run_multilabel_classifier.run(multilabel_param_grid, LogisticRegression(), comments_file=train_multilabel)\n",
    "with open('./saved_models/log_reg_joint_multilabel.pkl', 'wb') as saved_model:\n",
    "\tpickle.dump(multilabel_clf, file=saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.84      0.84      6242\n",
      "          1       0.84      0.84      0.84      6243\n",
      "\n",
      "avg / total       0.84      0.84      0.84     12485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_binary_test, y_binary_test = run_binary_classifier.load_comments(test_binary)\n",
    "y_binary_test_predict = binary_clf.predict(X_binary_test)\n",
    "print(classification_report(y_binary_test, y_binary_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6090\n",
      "1 367\n",
      "2 3691\n",
      "3 211\n",
      "4 3427\n",
      "5 712\n",
      "0 2416\n",
      "[6090  367 3691  211 3427  712]\n",
      "===================================\n",
      "0 3674\n",
      "1 2049\n",
      "2 1275\n",
      "3 2205\n",
      "4 3427\n",
      "5 1704\n",
      "===================================\n",
      "57735\n",
      "14334\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.49      1.00      0.66     14108\n",
      "          1       0.50      0.44      0.47      3466\n",
      "          2       0.51      0.94      0.66     10889\n",
      "          3       0.65      0.67      0.66      2880\n",
      "          4       0.43      0.98      0.60     11049\n",
      "          5       0.71      0.67      0.69      4142\n",
      "\n",
      "avg / total       0.51      0.89      0.63     46534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_multilabel_test, y_multilabel_test = run_multilabel_classifier.load_comments(test_multilabel)\n",
    "y_multilabel_test_predict = multilabel_clf.predict(X_multilabel_test)\n",
    "print(classification_report(y_multilabel_test, y_multilabel_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final joint prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mix type of y not allowed, got types {'multilabel-indicator', 'multiclass-multioutput'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-90d04c592479>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mfinal_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoxic_indices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultilabel_toxic_predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_multilabel_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1421\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mix type of y not allowed, got types %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mys_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mlabel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mys_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mix type of y not allowed, got types {'multilabel-indicator', 'multiclass-multioutput'}"
     ]
    }
   ],
   "source": [
    "final_predictions = np.full_like(y_multilabel_test_predict, -1)\n",
    "\n",
    "\n",
    "non_toxic_indices = np.argwhere(y_binary_test_predict == 0).flatten()\n",
    "toxic_indices = np.argwhere(y_binary_test_predict == 1).flatten()\n",
    "\n",
    "# place binary classifier's prediction of clean comments\n",
    "final_predictions[non_toxic_indices] = np.array([0, 0, 0, 0, 0, 0])\n",
    "\n",
    "multilabel_toxic_predictions = y_multilabel_test_predict[toxic_indices]\n",
    "final_predictions[toxic_indices] = multilabel_toxic_predictions\n",
    "\n",
    "print(classification_report(y_multilabel_test, final_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_param_grid = {\n",
    "        'bag_of_words__stop_words': ['english'],\n",
    "        'bag_of_words__ngram_range': [(1, 2)],\n",
    "        'bag_of_words__max_features': [500],\n",
    "        'dim_reduct__n_components': [300],\n",
    "        'normalizer__norm': ['l2'],\n",
    "        'classifier__alpha': [1.0],\n",
    "        'classifier__binarize': [0.0]\n",
    "}\n",
    "\n",
    "multilabel_param_grid = [{\n",
    "        'estimator__bag_of_words__stop_words': ['english'],\n",
    "        'estimator__bag_of_words__ngram_range': [(1, 2)],\n",
    "        'estimator__bag_of_words__max_features': [500],\n",
    "        'estimator__dim_reduct__n_components': [300],\n",
    "        'estimator__normalizer__norm': ['l2'],\n",
    "        'estimator__classifier__alpha': [1.0],\n",
    "        'estimator__classifier__binarize': [0.0]\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_clf = run_binary_classifier.run(binary_param_grid, BernoulliNB(), comments_file=train_binary)\n",
    "\n",
    "with open('./saved_models/naiveB_joint_binary.pkl', 'wb') as saved_model:\n",
    "\tpickle.dump(binary_clf, file=saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_clf = run_multilabel_classifier.run(multilabel_param_grid, BernoulliNB(), comments_file=train_multilabel)\n",
    "with open('./saved_models/naiveB_joint_binary.pkl', 'wb') as saved_model:\n",
    "\tpickle.dump(binary_clf, file=saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_binary_test, y_binary_test = run_binary_classifier.load_comments(test_binary)\n",
    "y_binary_test_predict = binary_clf.predict(X_binary_test)\n",
    "\n",
    "print(classification_report(y_binary_test, y_binary_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_multilabel_test, y_multilabel_test = run_multilabel_classifier.load_comments(test_multilabel)\n",
    "y_multilabel_test_predict = multilabel_clf.predict(X_multilabel_test)\n",
    "\n",
    "print(classification_report(y_multilabel_test, y_multilabel_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final joint prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = np.full_like(y_multilabel_test_predict, -1)\n",
    "\n",
    "\n",
    "non_toxic_indices = np.argwhere(y_binary_test_predict == 0).flatten()\n",
    "toxic_indices = np.argwhere(y_binary_test_predict == 1).flatten()\n",
    "\n",
    "# place binary classifier's prediction of clean comments\n",
    "final_predictions[non_toxic_indices] = np.array([0, 0, 0, 0, 0, 0])\n",
    "\n",
    "multilabel_toxic_predictions = y_multilabel_test_predict[toxic_indices]\n",
    "final_predictions[toxic_indices] = multilabel_toxic_predictions\n",
    "\n",
    "print(classification_report(y_multilabel_test, final_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_param_grid = {\n",
    "        'bag_of_words__stop_words': ['english'],\n",
    "        'bag_of_words__ngram_range': [(1, 2)],\n",
    "        'bag_of_words__max_features': [500],\n",
    "        'dim_reduct__n_components': [300],\n",
    "        'normalizer__norm': ['l2'],\n",
    "        'classifier__max_depth': [5, 10, 15]\n",
    "}\n",
    "\n",
    "multilabel_param_grid = [{\n",
    "        'estimator__bag_of_words__stop_words': ['english'],\n",
    "        'estimator__bag_of_words__ngram_range': [(1, 2)],\n",
    "        'estimator__bag_of_words__max_features': [500],\n",
    "        'estimator__dim_reduct__n_components': [300],\n",
    "        'estimator__normalizer__norm': ['l2'],\n",
    "        'estimator__classifier__max_depth': [5, 10, 15]\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_clf = run_binary_classifier.run(binary_param_grid, DecisionTreeClassifier(), comments_file=train_binary)\n",
    "\n",
    "with open('./saved_models/dec_tree_joint_binary.pkl', 'wb') as saved_model:\n",
    "\tpickle.dump(binary_clf, file=saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_clf = run_multilabel_classifier.run(multilabel_param_grid, DecisionTreeClassifier(), comments_file=train_multilabel)\n",
    "with open('./saved_models/dec_tree_joint_binary.pkl', 'wb') as saved_model:\n",
    "\tpickle.dump(binary_clf, file=saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_binary_test, y_binary_test = run_binary_classifier.load_comments(test_binary)\n",
    "y_binary_test_predict = binary_clf.predict(X_binary_test)\n",
    "\n",
    "print(classification_report(y_binary_test, y_binary_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict multilabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_multilabel_test, y_multilabel_test = run_multilabel_classifier.load_comments(test_multilabel)\n",
    "y_multilabel_test_predict = multilabel_clf.predict(X_multilabel_test)\n",
    "\n",
    "print(classification_report(y_multilabel_test, y_multilabel_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final joint prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = np.full_like(y_multilabel_test_predict, -1)\n",
    "\n",
    "\n",
    "non_toxic_indices = np.argwhere(y_binary_test_predict == 0).flatten()\n",
    "toxic_indices = np.argwhere(y_binary_test_predict == 1).flatten()\n",
    "\n",
    "# place binary classifier's prediction of clean comments\n",
    "final_predictions[non_toxic_indices] = np.array([0, 0, 0, 0, 0, 0])\n",
    "\n",
    "multilabel_toxic_predictions = y_multilabel_test_predict[toxic_indices]\n",
    "final_predictions[toxic_indices] = multilabel_toxic_predictions\n",
    "\n",
    "print(classification_report(y_multilabel_test, final_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
