{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from run_multilabel_classifier import _load_comments, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_comments_path = os.path.join('../', 'data/train.csv')\n",
    "test_comments_path = os.path.join('../', 'data/test_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{\n",
    "        'estimator__bag_of_words__stop_words': ['english'],\n",
    "        'estimator__bag_of_words__ngram_range': [(1, 2)],\n",
    "        'estimator__bag_of_words__max_features': [500],\n",
    "        'estimator__dim_reduct__n_components': [300],\n",
    "        'estimator__normalizer__norm': ['l2'],\n",
    "        'estimator__classifier__C': [10., 100.]\n",
    "}]\n",
    "\n",
    "\n",
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 15294\n",
      "1 1595\n",
      "2 8449\n",
      "3 478\n",
      "4 7877\n",
      "5 1405\n",
      "0 5849\n",
      "[15294  1595  8449   478  7877  1405]\n",
      "===================================\n",
      "0 9445\n",
      "1 4254\n",
      "2 8449\n",
      "3 5371\n",
      "4 7877\n",
      "5 4444\n",
      "===================================\n",
      "143346\n",
      "39840\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jasmi\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 17.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([76.8087019 , 83.87642398]),\n",
      " 'mean_score_time': array([4.23123312, 4.25043154]),\n",
      " 'mean_test_score': array([0.64150605, 0.64147019]),\n",
      " 'mean_train_score': array([0.64772299, 0.6480771 ]),\n",
      " 'param_estimator__bag_of_words__max_features': masked_array(data=[500, 500],\n",
      "             mask=[False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'param_estimator__bag_of_words__ngram_range': masked_array(data=[(1, 2), (1, 2)],\n",
      "             mask=[False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'param_estimator__bag_of_words__stop_words': masked_array(data=['english', 'english'],\n",
      "             mask=[False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'param_estimator__classifier__C': masked_array(data=[10.0, 100.0],\n",
      "             mask=[False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'param_estimator__dim_reduct__n_components': masked_array(data=[300, 300],\n",
      "             mask=[False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'param_estimator__normalizer__norm': masked_array(data=['l2', 'l2'],\n",
      "             mask=[False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'params': [{'estimator__bag_of_words__max_features': 500,\n",
      "             'estimator__bag_of_words__ngram_range': (1, 2),\n",
      "             'estimator__bag_of_words__stop_words': 'english',\n",
      "             'estimator__classifier__C': 10.0,\n",
      "             'estimator__dim_reduct__n_components': 300,\n",
      "             'estimator__normalizer__norm': 'l2'},\n",
      "            {'estimator__bag_of_words__max_features': 500,\n",
      "             'estimator__bag_of_words__ngram_range': (1, 2),\n",
      "             'estimator__bag_of_words__stop_words': 'english',\n",
      "             'estimator__classifier__C': 100.0,\n",
      "             'estimator__dim_reduct__n_components': 300,\n",
      "             'estimator__normalizer__norm': 'l2'}],\n",
      " 'rank_test_score': array([1, 2]),\n",
      " 'split0_test_score': array([0.64249216, 0.64294039]),\n",
      " 'split0_train_score': array([0.64834155, 0.64955177]),\n",
      " 'split1_test_score': array([0.65073958, 0.64885701]),\n",
      " 'split1_train_score': array([0.64547288, 0.64576423]),\n",
      " 'split2_test_score': array([0.64078888, 0.64195428]),\n",
      " 'split2_train_score': array([0.64728821, 0.64778126]),\n",
      " 'split3_test_score': array([0.63792022, 0.63783057]),\n",
      " 'split3_train_score': array([0.64798297, 0.64757956]),\n",
      " 'split4_test_score': array([0.63558942, 0.63576871]),\n",
      " 'split4_train_score': array([0.64952936, 0.64970865]),\n",
      " 'std_fit_time': array([1.33637443, 1.81175819]),\n",
      " 'std_score_time': array([0.05632762, 0.04138832]),\n",
      " 'std_test_score': array([0.00518863, 0.00453243]),\n",
      " 'std_train_score': array([0.00133904, 0.00145059])}\n",
      "=================  Classification report  =================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.84      0.88     11459\n",
      "          1       0.63      0.44      0.52      3170\n",
      "          2       0.93      0.83      0.88      9032\n",
      "          3       0.81      0.56      0.66      2058\n",
      "          4       0.85      0.78      0.82      8785\n",
      "          5       0.77      0.47      0.58      2814\n",
      "\n",
      "avg / total       0.87      0.75      0.80     37318\n",
      "\n",
      "=================     Best parameters     =================\n",
      "{'estimator__bag_of_words__max_features': 500,\n",
      " 'estimator__bag_of_words__ngram_range': (1, 2),\n",
      " 'estimator__bag_of_words__stop_words': 'english',\n",
      " 'estimator__classifier__C': 10.0,\n",
      " 'estimator__dim_reduct__n_components': 300,\n",
      " 'estimator__normalizer__norm': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "multilabel_clf = run(param_grid, clf, comments_file=train_comments_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./saved_models/logreg_trained_mlutils.pkl', 'wb') as saved_model:\n",
    "    pickle.dump(multilabel_clf, file=saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the model and testing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneVsRestClassifier(estimator=Pipeline(memory=None,\n",
      "     steps=[('bag_of_words', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=T...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))]),\n",
      "          n_jobs=1)\n",
      "0 6090\n",
      "1 367\n",
      "2 3691\n",
      "3 211\n",
      "4 3427\n",
      "5 712\n",
      "0 2416\n",
      "[6090  367 3691  211 3427  712]\n",
      "===================================\n",
      "0 3674\n",
      "1 2049\n",
      "2 1275\n",
      "3 2205\n",
      "4 3427\n",
      "5 1704\n",
      "===================================\n",
      "57735\n",
      "14334\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.86      0.88     14102\n",
      "          1       0.57      0.44      0.50      3528\n",
      "          2       0.87      0.81      0.84     10848\n",
      "          3       0.76      0.61      0.68      2884\n",
      "          4       0.82      0.78      0.80     11030\n",
      "          5       0.82      0.54      0.65      4179\n",
      "\n",
      "avg / total       0.83      0.75      0.79     46571\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('./saved_models/logreg_trained_mlutils.pkl', 'rb') as saved_model:\n",
    "    loaded_clf = pickle.load(saved_model)\n",
    "    print(loaded_clf)\n",
    "    X_test, y_test = _load_comments(test_comments_path)\n",
    "    y_test_predict = loaded_clf.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{\n",
    "        'estimator__bag_of_words__stop_words': ['english'],\n",
    "        'estimator__bag_of_words__ngram_range': [(1, 2)],\n",
    "        'estimator__bag_of_words__max_features': [500],\n",
    "        'estimator__dim_reduct__n_components': [300],\n",
    "        'estimator__normalizer__norm': ['l2'],\n",
    "        'estimator__classifier__alpha': [1.0],\n",
    "        'estimator__classifier__binarize': [0.0]\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 15294\n",
      "1 1595\n",
      "2 8449\n",
      "3 478\n",
      "4 7877\n",
      "5 1405\n",
      "0 5849\n",
      "[15294  1595  8449   478  7877  1405]\n",
      "===================================\n",
      "0 9445\n",
      "1 4254\n",
      "2 8449\n",
      "3 5371\n",
      "4 7877\n",
      "5 4444\n",
      "===================================\n",
      "143346\n",
      "39840\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jasmi\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  7.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([63.33680687]),\n",
      " 'mean_score_time': array([5.07463994]),\n",
      " 'mean_test_score': array([0.50657104]),\n",
      " 'mean_train_score': array([0.50900493]),\n",
      " 'param_estimator__bag_of_words__max_features': masked_array(data=[500],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'param_estimator__bag_of_words__ngram_range': masked_array(data=[(1, 2)],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'param_estimator__bag_of_words__stop_words': masked_array(data=['english'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'param_estimator__classifier__alpha': masked_array(data=[1.0],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'param_estimator__classifier__binarize': masked_array(data=[0.0],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'param_estimator__dim_reduct__n_components': masked_array(data=[300],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'param_estimator__normalizer__norm': masked_array(data=['l2'],\n",
      "             mask=[False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'params': [{'estimator__bag_of_words__max_features': 500,\n",
      "             'estimator__bag_of_words__ngram_range': (1, 2),\n",
      "             'estimator__bag_of_words__stop_words': 'english',\n",
      "             'estimator__classifier__alpha': 1.0,\n",
      "             'estimator__classifier__binarize': 0.0,\n",
      "             'estimator__dim_reduct__n_components': 300,\n",
      "             'estimator__normalizer__norm': 'l2'}],\n",
      " 'rank_test_score': array([1]),\n",
      " 'split0_test_score': array([0.5015688]),\n",
      " 'split0_train_score': array([0.50616316]),\n",
      " 'split1_test_score': array([0.51286419]),\n",
      " 'split1_train_score': array([0.50609592]),\n",
      " 'split2_test_score': array([0.5070372]),\n",
      " 'split2_train_score': array([0.50784402]),\n",
      " 'split3_test_score': array([0.5070372]),\n",
      " 'split3_train_score': array([0.51315554]),\n",
      " 'split4_test_score': array([0.50434783]),\n",
      " 'split4_train_score': array([0.51176602]),\n",
      " 'std_fit_time': array([2.49433346]),\n",
      " 'std_score_time': array([0.29690232]),\n",
      " 'std_test_score': array([0.00374175]),\n",
      " 'std_train_score': array([0.00292359])}\n",
      "=================  Classification report  =================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.77      0.79     11489\n",
      "          1       0.42      0.58      0.49      3208\n",
      "          2       0.78      0.78      0.78      9034\n",
      "          3       0.51      0.51      0.51      2076\n",
      "          4       0.73      0.74      0.74      8780\n",
      "          5       0.44      0.48      0.46      2802\n",
      "\n",
      "avg / total       0.71      0.71      0.71     37389\n",
      "\n",
      "=================     Best parameters     =================\n",
      "{'estimator__bag_of_words__max_features': 500,\n",
      " 'estimator__bag_of_words__ngram_range': (1, 2),\n",
      " 'estimator__bag_of_words__stop_words': 'english',\n",
      " 'estimator__classifier__alpha': 1.0,\n",
      " 'estimator__classifier__binarize': 0.0,\n",
      " 'estimator__dim_reduct__n_components': 300,\n",
      " 'estimator__normalizer__norm': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "multilabel_clf = run(param_grid, BernoulliNB(), comments_file=train_comments_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./saved_models/naiveB_multilabel.pkl', 'wb') as saved_model:\n",
    "\tpickle.dump(multilabel_clf, file=saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the model and testing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneVsRestClassifier(estimator=Pipeline(memory=None,\n",
      "     steps=[('bag_of_words', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=T...norm='l2')), ('classifier', BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True))]),\n",
      "          n_jobs=1)\n",
      "0 6090\n",
      "1 367\n",
      "2 3691\n",
      "3 211\n",
      "4 3427\n",
      "5 712\n",
      "0 2416\n",
      "[6090  367 3691  211 3427  712]\n",
      "===================================\n",
      "0 3674\n",
      "1 2049\n",
      "2 1275\n",
      "3 2205\n",
      "4 3427\n",
      "5 1704\n",
      "===================================\n",
      "57735\n",
      "14334\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.80      0.81     14103\n",
      "          1       0.35      0.64      0.45      3538\n",
      "          2       0.74      0.80      0.77     10884\n",
      "          3       0.51      0.63      0.57      2891\n",
      "          4       0.73      0.77      0.75     11044\n",
      "          5       0.44      0.55      0.49      4087\n",
      "\n",
      "avg / total       0.69      0.75      0.72     46547\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('./saved_models/naiveB_multilabel.pkl', 'rb') as saved_model:\n",
    "    loaded_clf = pickle.load(saved_model)\n",
    "    print(loaded_clf)\n",
    "    X_test, y_test = _load_comments(test_comments_path)\n",
    "    y_test_predict = loaded_clf.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_test_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [{\n",
    "        'estimator__bag_of_words__stop_words': ['english'],\n",
    "        'estimator__bag_of_words__ngram_range': [(1, 2)],\n",
    "        'estimator__bag_of_words__max_features': [500],\n",
    "        'estimator__dim_reduct__n_components': [300],\n",
    "        'estimator__normalizer__norm': ['l2'],\n",
    "        'estimator__classifier__max_depth': [5, 10, 15]\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 15294\n",
      "1 1595\n",
      "2 8449\n",
      "3 478\n",
      "4 7877\n",
      "5 1405\n",
      "0 5849\n",
      "[15294  1595  8449   478  7877  1405]\n",
      "===================================\n",
      "0 9445\n",
      "1 4254\n",
      "2 8449\n",
      "3 5371\n",
      "4 7877\n",
      "5 4444\n",
      "===================================\n",
      "143346\n",
      "39840\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jasmi\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed: 38.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([100.42878318, 131.28491473, 162.20315762]),\n",
      " 'mean_score_time': array([4.52870488, 4.32165112, 4.42007117]),\n",
      " 'mean_test_score': array([0.60455401, 0.66752129, 0.71788436]),\n",
      " 'mean_train_score': array([0.61279695, 0.71593008, 0.7996459 ]),\n",
      " 'param_estimator__bag_of_words__max_features': masked_array(data=[500, 500, 500],\n",
      "             mask=[False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'param_estimator__bag_of_words__ngram_range': masked_array(data=[(1, 2), (1, 2), (1, 2)],\n",
      "             mask=[False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'param_estimator__bag_of_words__stop_words': masked_array(data=['english', 'english', 'english'],\n",
      "             mask=[False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'param_estimator__classifier__max_depth': masked_array(data=[5, 10, 15],\n",
      "             mask=[False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'param_estimator__dim_reduct__n_components': masked_array(data=[300, 300, 300],\n",
      "             mask=[False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'param_estimator__normalizer__norm': masked_array(data=['l2', 'l2', 'l2'],\n",
      "             mask=[False, False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object),\n",
      " 'params': [{'estimator__bag_of_words__max_features': 500,\n",
      "             'estimator__bag_of_words__ngram_range': (1, 2),\n",
      "             'estimator__bag_of_words__stop_words': 'english',\n",
      "             'estimator__classifier__max_depth': 5,\n",
      "             'estimator__dim_reduct__n_components': 300,\n",
      "             'estimator__normalizer__norm': 'l2'},\n",
      "            {'estimator__bag_of_words__max_features': 500,\n",
      "             'estimator__bag_of_words__ngram_range': (1, 2),\n",
      "             'estimator__bag_of_words__stop_words': 'english',\n",
      "             'estimator__classifier__max_depth': 10,\n",
      "             'estimator__dim_reduct__n_components': 300,\n",
      "             'estimator__normalizer__norm': 'l2'},\n",
      "            {'estimator__bag_of_words__max_features': 500,\n",
      "             'estimator__bag_of_words__ngram_range': (1, 2),\n",
      "             'estimator__bag_of_words__stop_words': 'english',\n",
      "             'estimator__classifier__max_depth': 15,\n",
      "             'estimator__dim_reduct__n_components': 300,\n",
      "             'estimator__normalizer__norm': 'l2'}],\n",
      " 'rank_test_score': array([3, 2, 1]),\n",
      " 'split0_test_score': array([0.59910354, 0.65674585, 0.71653967]),\n",
      " 'split0_train_score': array([0.60999552, 0.70596145, 0.79363514]),\n",
      " 'split1_test_score': array([0.60851636, 0.67225459, 0.7224563 ]),\n",
      " 'split1_train_score': array([0.6119229 , 0.71842223, 0.80190498]),\n",
      " 'split2_test_score': array([0.60233079, 0.67404751, 0.71510533]),\n",
      " 'split2_train_score': array([0.60932317, 0.71745854, 0.79659346]),\n",
      " 'split3_test_score': array([0.6087853 , 0.66705513, 0.72093232]),\n",
      " 'split3_train_score': array([0.61954281, 0.71678619, 0.80280143]),\n",
      " 'split4_test_score': array([0.60403407, 0.66750336, 0.71438817]),\n",
      " 'split4_train_score': array([0.61320036, 0.72102196, 0.80329449]),\n",
      " 'std_fit_time': array([4.30957083, 1.95595871, 3.13953143]),\n",
      " 'std_score_time': array([0.21107465, 0.08016418, 0.29091644]),\n",
      " 'std_test_score': array([0.00370202, 0.00602202, 0.00322327]),\n",
      " 'std_train_score': array([0.00364276, 0.00518808, 0.00384239])}\n",
      "=================  Classification report  =================\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.84      0.88     11496\n",
      "          1       0.87      0.73      0.80      3176\n",
      "          2       0.93      0.87      0.90      9072\n",
      "          3       0.90      0.83      0.87      2029\n",
      "          4       0.87      0.85      0.86      8758\n",
      "          5       0.88      0.62      0.73      2835\n",
      "\n",
      "avg / total       0.90      0.82      0.86     37366\n",
      "\n",
      "=================     Best parameters     =================\n",
      "{'estimator__bag_of_words__max_features': 500,\n",
      " 'estimator__bag_of_words__ngram_range': (1, 2),\n",
      " 'estimator__bag_of_words__stop_words': 'english',\n",
      " 'estimator__classifier__max_depth': 15,\n",
      " 'estimator__dim_reduct__n_components': 300,\n",
      " 'estimator__normalizer__norm': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "multilabel_clf = run(param_grid, DecisionTreeClassifier(), comments_file=train_comments_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./saved_models/dec_tree_multilabel.pkl', 'wb') as saved_model:\n",
    "\tpickle.dump(multilabel_clf, file=saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the model and testing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneVsRestClassifier(estimator=Pipeline(memory=None,\n",
      "     steps=[('bag_of_words', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=500, min_df=1,\n",
      "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=T...      min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'))]),\n",
      "          n_jobs=1)\n",
      "0 6090\n",
      "1 367\n",
      "2 3691\n",
      "3 211\n",
      "4 3427\n",
      "5 712\n",
      "0 2416\n",
      "[6090  367 3691  211 3427  712]\n",
      "===================================\n",
      "0 3674\n",
      "1 2049\n",
      "2 1275\n",
      "3 2205\n",
      "4 3427\n",
      "5 1704\n",
      "===================================\n",
      "57735\n",
      "14334\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.79      0.84     14124\n",
      "          1       0.45      0.31      0.37      3474\n",
      "          2       0.84      0.76      0.80     10942\n",
      "          3       0.69      0.38      0.49      2873\n",
      "          4       0.80      0.70      0.75     11120\n",
      "          5       0.71      0.37      0.48      4105\n",
      "\n",
      "avg / total       0.80      0.66      0.72     46638\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('./saved_models/dec_tree_multilabel.pkl', 'rb') as saved_model:\n",
    "    loaded_clf = pickle.load(saved_model)\n",
    "    print(loaded_clf)\n",
    "    X_test, y_test = _load_comments(test_comments_path)\n",
    "    y_test_predict = loaded_clf.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_test_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
